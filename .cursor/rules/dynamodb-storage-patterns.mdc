---
title: DynamoDB Storage Best Practices
description: Best practices for storing and retrieving data in DynamoDB, with a focus on preventing race conditions and ensuring data integrity
---

# DynamoDB Storage Best Practices

This rule provides guidance on best practices for storing and retrieving data in DynamoDB, with a focus on preventing race conditions and ensuring data integrity.

## Rule

```json
{
  "name": "dynamodb_storage_patterns",
  "description": "Best practices for storing and retrieving data in DynamoDB",
  "priority": "high",
  "version": "1.1",
  "filters": [
    {
      "extension": [".py", ".js", ".ts"],
      "content_patterns": ["dynamodb", "DynamoDB", "put_item", "update_item", "batch_write", "service_level", "capabilities"]
    }
  ],
  "actions": [
    {
      "pattern": "time\\.time\\(\\)",
      "suggestion": "Consider using a more precise timestamp with millisecond precision (time.time() * 1000) for generating unique IDs",
      "explanation": "Using millisecond precision helps prevent ID collisions in high-throughput scenarios"
    },
    {
      "pattern": "conversationId.*customer_id",
      "suggestion": "Ensure each record has a unique identifier by combining entity type, timestamp, and a suffix",
      "explanation": "Using composite IDs (e.g., `conv_{customer_id}_{entity_type}_{timestamp}`) prevents race conditions"
    },
    {
      "pattern": "put_item\\(Item=\\{[^}]*\\}\\)",
      "suggestion": "Add error handling around DynamoDB operations and include a unique ID field",
      "explanation": "Error handling prevents silent failures, and unique IDs prevent record collisions"
    },
    {
      "pattern": "batch_write_item\\(",
      "suggestion": "Implement retry logic for batch operations and handle partial failures",
      "explanation": "Batch operations can partially succeed, requiring careful error handling"
    },
    {
      "pattern": "device\\s*=\\s*\\{[^}]*\\}",
      "suggestion": "Ensure device data includes all required fields based on service level",
      "explanation": "Device data structure should adapt based on the customer's service level"
    },
    {
      "pattern": "capabilities\\s*=\\s*\\[[^\\]]*\\]",
      "suggestion": "Include service-level-specific capabilities in the correct order",
      "explanation": "Capabilities should be ordered from basic to advanced (e.g., device_status, device_power, volume_control, song_changes)"
    }
  ]
}
```

## Best Practices

### 1. Use Unique and Composite Primary Keys

**Bad Practice:**
```python
# Using only customer_id as the key can lead to overwrites
conversation_id = f"conv_{customer_id}"
dynamodb_table.put_item(Item={
    'conversationId': conversation_id,
    'userId': customer_id,
    'message': message
})
```

**Good Practice:**
```python
# Using a composite key with timestamp and type ensures uniqueness
timestamp_ms = int(time.time() * 1000)
message_id = f"msg_{customer_id}_{timestamp_ms}"
conversation_id = f"conv_{customer_id}_{message_type}_{timestamp_ms}"
dynamodb_table.put_item(Item={
    'id': message_id,  # Unique ID as primary key
    'conversationId': conversation_id,
    'timestamp': datetime.utcnow().isoformat(),
    'userId': customer_id,
    'message': message,
    'type': message_type
})
```

### 2. Implement Proper Error Handling

**Bad Practice:**
```python
# No error handling can lead to silent failures
dynamodb_table.put_item(Item={
    'id': item_id,
    'data': item_data
})
```

**Good Practice:**
```python
try:
    dynamodb_table.put_item(Item={
        'id': item_id,
        'data': item_data
    })
except Exception as e:
    logger.error(f"Failed to store item {item_id} in DynamoDB: {str(e)}")
    # Implement appropriate fallback or retry logic
```

### 3. Use Conditional Writes for Concurrency Control

**Bad Practice:**
```python
# Unconditional writes can lead to lost updates
dynamodb_table.update_item(
    Key={'id': item_id},
    UpdateExpression="SET #count = #count + :inc",
    ExpressionAttributeNames={"#count": "count"},
    ExpressionAttributeValues={":inc": 1}
)
```

**Good Practice:**
```python
try:
    # Use conditional expressions to prevent lost updates
    dynamodb_table.update_item(
        Key={'id': item_id},
        UpdateExpression="SET #count = #count + :inc",
        ConditionExpression="attribute_exists(id)",
        ExpressionAttributeNames={"#count": "count"},
        ExpressionAttributeValues={":inc": 1}
    )
except dynamodb.exceptions.ConditionalCheckFailedException:
    logger.warning(f"Item {item_id} does not exist, creating new item")
    dynamodb_table.put_item(Item={
        'id': item_id,
        'count': 1
    })
```

### 4. Implement Retry Logic with Exponential Backoff

**Bad Practice:**
```python
# Single attempt without retries can fail due to throttling
response = dynamodb_table.query(
    KeyConditionExpression=Key('userId').eq(user_id)
)
```

**Good Practice:**
```python
def query_with_retry(user_id, max_attempts=3):
    attempt = 0
    while attempt < max_attempts:
        try:
            response = dynamodb_table.query(
                KeyConditionExpression=Key('userId').eq(user_id)
            )
            return response
        except dynamodb.exceptions.ProvisionedThroughputExceededException:
            wait_time = (2 ** attempt) * 0.1  # Exponential backoff
            logger.warning(f"Throttled, retrying in {wait_time}s")
            time.sleep(wait_time)
            attempt += 1
    
    # If we've exhausted retries, log and handle the failure
    logger.error(f"Failed to query after {max_attempts} attempts")
    return None
```

### 5. Use Batch Operations Efficiently

**Bad Practice:**
```python
# Individual writes for multiple items is inefficient
for item in items:
    dynamodb_table.put_item(Item=item)
```

**Good Practice:**
```python
def batch_write_with_retry(items, table_name, max_attempts=3):
    # DynamoDB limits batch size to 25 items
    batch_size = 25
    for i in range(0, len(items), batch_size):
        batch_items = items[i:i+batch_size]
        request_items = {
            table_name: [{'PutRequest': {'Item': item}} for item in batch_items]
        }
        
        attempt = 0
        unprocessed_items = request_items
        
        while unprocessed_items and attempt < max_attempts:
            response = dynamodb.batch_write_item(RequestItems=unprocessed_items)
            unprocessed_items = response.get('UnprocessedItems', {})
            
            if unprocessed_items:
                wait_time = (2 ** attempt) * 0.1
                logger.warning(f"Unprocessed items, retrying in {wait_time}s")
                time.sleep(wait_time)
                attempt += 1
        
        if unprocessed_items:
            logger.error(f"Failed to process {len(unprocessed_items)} items after {max_attempts} attempts")
```

### 6. Ensure Idempotent Operations

**Bad Practice:**
```python
# Non-idempotent operation can cause duplicate processing
def process_message(message_id):
    # Process the message
    mark_as_processed(message_id)
```

**Good Practice:**
```python
def process_message(message_id):
    # Check if already processed
    response = dynamodb_table.get_item(
        Key={'id': message_id},
        ProjectionExpression='processed'
    )
    
    if response.get('Item', {}).get('processed', False):
        logger.info(f"Message {message_id} already processed, skipping")
        return
    
    # Process the message
    try:
        # Actual processing logic
        
        # Mark as processed with condition to ensure idempotency
        dynamodb_table.update_item(
            Key={'id': message_id},
            UpdateExpression="SET processed = :true",
            ConditionExpression="attribute_not_exists(processed) OR processed = :false",
            ExpressionAttributeValues={
                ':true': True,
                ':false': False
            }
        )
    except dynamodb.exceptions.ConditionalCheckFailedException:
        logger.warning(f"Message {message_id} processed by another instance")
```

### 7. Service-Level-Specific Data Modeling

When storing data that varies based on service levels (e.g., basic, premium, enterprise), follow these guidelines:

#### Base Data Structure
Always include the base fields that are common across all service levels:

**Good Practice:**
```python
def create_device(customer_id: str, device_type: str, service_level: str) -> Dict[str, Any]:
    # Base device structure - required for all service levels
    device = {
        'id': f"{customer_id}-device-1",
        'type': device_type,
        'state': 'off',
        'location': 'living_room',
        'capabilities': ["device_status", "device_power"]  # Basic capabilities
    }
    return device
```

#### Conditional Fields Based on Service Level
Add fields conditionally based on service level to avoid unnecessary data:

**Bad Practice:**
```python
# Including all fields regardless of service level
device = {
    'id': device_id,
    'type': device_type,
    'state': 'off',
    'location': location,
    'capabilities': capabilities,
    'volume': 50,  # Not needed for basic level
    'currentSong': 'No song playing'  # Not needed for basic/premium
}
```

**Good Practice:**
```python
def create_device_with_capabilities(customer_id: str, device_type: str, service_level: str) -> Dict[str, Any]:
    # Start with base device structure
    device = {
        'id': f"{customer_id}-device-1",
        'type': device_type,
        'state': 'off',
        'location': 'living_room',
        'capabilities': ["device_status", "device_power"]  # Basic capabilities
    }
    
    # Add premium features
    if service_level in ['premium', 'enterprise']:
        device['capabilities'].append("volume_control")
        device['volume'] = 50  # Default volume level
    
    # Add enterprise features
    if service_level == 'enterprise':
        device['capabilities'].append("song_changes")
        device['currentSong'] = 'No song playing'
    
    return device
```

#### Service Level Validation
Implement validation to ensure data consistency with service levels:

**Good Practice:**
```python
def validate_device_data(device: Dict[str, Any], service_level: str) -> bool:
    """Validate device data matches service level capabilities."""
    required_fields = {'id', 'type', 'state', 'location', 'capabilities'}
    
    # Check base fields
    if not all(field in device for field in required_fields):
        return False
        
    # Validate capabilities match service level
    capabilities = device.get('capabilities', [])
    
    if service_level == 'basic':
        allowed_capabilities = {"device_status", "device_power"}
        if not set(capabilities).issubset(allowed_capabilities):
            return False
            
    elif service_level == 'premium':
        if 'volume' not in device or 'volume_control' not in capabilities:
            return False
            
    elif service_level == 'enterprise':
        if ('currentSong' not in device or 
            'song_changes' not in capabilities or
            'volume_control' not in capabilities):
            return False
            
    return True
```

#### Testing Service Level Data
Include tests for each service level's data structure:

**Good Practice:**
```python
def test_device_data_structure():
    # Test basic level device
    basic_device = create_device_with_capabilities("cust1", "speaker", "basic")
    assert "volume" not in basic_device
    assert "currentSong" not in basic_device
    assert set(basic_device['capabilities']) == {"device_status", "device_power"}
    
    # Test premium level device
    premium_device = create_device_with_capabilities("cust2", "speaker", "premium")
    assert "volume" in premium_device
    assert "currentSong" not in premium_device
    assert "volume_control" in premium_device['capabilities']
    
    # Test enterprise level device
    enterprise_device = create_device_with_capabilities("cust3", "speaker", "enterprise")
    assert "volume" in enterprise_device
    assert "currentSong" in enterprise_device
    assert "song_changes" in enterprise_device['capabilities']
```

### Benefits of Service-Level Data Modeling

1. **Data Efficiency**: Only store fields that are relevant to each service level
2. **Reduced Errors**: Prevent accidental access to features not available at certain service levels
3. **Clear Capabilities**: Explicit capability lists make it easy to check feature availability
4. **Upgrade Path**: Clear structure for adding features when customers upgrade
5. **Consistent Validation**: Easy to validate data against service level requirements 