# Comprehensive Logging Best Practices

This rule enforces best practices for implementing comprehensive logging in Python applications.

<rule>
name: comprehensive_logging
description: Enforce best practices for comprehensive logging in Python
filters:
  - type: file_extension
    pattern: "\\.py$"
  - type: content
    pattern: "(?:logging|logger)"

actions:
  - type: suggest
    conditions:
      # If logging without context
      - pattern: "logger\\.[a-z]+\\(['\"][^'\"]+['\"]\\)"
        message: "Include context information in log messages"
      # If not logging exceptions
      - pattern: "except\\s+[a-zA-Z0-9_]+(?:\\s+as\\s+([a-zA-Z0-9_]+))?\\s*:(?!.*logger)"
        message: "Log exceptions with appropriate context"
      # If not logging function entry/exit
      - pattern: "def\\s+[a-zA-Z0-9_]+\\s*\\([^)]*\\)\\s*(?:->\\s*[^:]+)?\\s*:(?!.*logger)"
        message: "Consider logging function entry/exit for important functions"
    message: |
      ## Comprehensive Logging Best Practices

      Follow these guidelines for implementing comprehensive logging:

      1. **Contextual Information**
         - Include relevant context in all log messages
         - Log user/customer IDs to trace activity across the system
         - Include operation IDs or request IDs for correlation
         - Log timestamps for timing analysis
      
      2. **Appropriate Log Levels**
         - Use DEBUG for detailed troubleshooting information
         - Use INFO for general operational information
         - Use WARNING for potential issues that don't prevent operation
         - Use ERROR for issues that prevent normal operation
         - Use CRITICAL for severe issues that require immediate attention
      
      3. **Structured Logging**
         - Use structured logging formats (JSON) for machine parsing
         - Include consistent fields across log messages
         - Use key-value pairs for important attributes
         - Consider using a logging framework that supports structured logging
      
      4. **Performance Metrics**
         - Log timing information for critical operations
         - Track resource usage (memory, CPU) for performance-sensitive code
         - Log counts of processed items for throughput analysis
         - Use sampling for high-volume operations
      
      5. **Error Details**
         - Log full exception details including stack traces
         - Include context about what led to the error
         - Log all relevant variables for debugging
         - Avoid logging sensitive information (passwords, tokens)
      
      6. **Lifecycle Events**
         - Log application startup and shutdown
         - Log important state transitions
         - Log connection establishment and termination
         - Log user authentication and authorization events

      ### Example:
      ```python
      import logging
      import time
      import uuid
      from typing import Dict, Any
      
      logger = logging.getLogger(__name__)
      
      def process_customer_request(customer_id: str, request_data: Dict[str, Any]) -> Dict[str, Any]:
          """Process a customer request.
          
          Args:
              customer_id: The ID of the customer
              request_data: The request data to process
              
          Returns:
              The processed response
          """
          request_id = str(uuid.uuid4())
          logger.info(f"Processing request {request_id} for customer {customer_id}", 
                     extra={"request_id": request_id, "customer_id": customer_id})
          
          start_time = time.time()
          
          try:
              # Validate request
              if not request_data.get("action"):
                  logger.warning(f"Missing action in request {request_id} for customer {customer_id}",
                               extra={"request_id": request_id, "customer_id": customer_id})
                  return {"error": "Missing action"}
              
              # Process request
              action = request_data["action"]
              logger.info(f"Executing action '{action}' for request {request_id}",
                         extra={"request_id": request_id, "customer_id": customer_id, "action": action})
              
              result = execute_action(action, request_data)
              
              processing_time = time.time() - start_time
              logger.info(f"Request {request_id} processed in {processing_time:.2f} seconds",
                         extra={"request_id": request_id, "customer_id": customer_id, 
                                "processing_time": processing_time})
              
              return result
          except ValueError as e:
              processing_time = time.time() - start_time
              logger.error(f"Validation error processing request {request_id}: {str(e)}",
                          extra={"request_id": request_id, "customer_id": customer_id, 
                                 "error": str(e), "processing_time": processing_time})
              return {"error": f"Validation error: {str(e)}"}
          except Exception as e:
              processing_time = time.time() - start_time
              logger.error(f"Unexpected error processing request {request_id}: {str(e)}",
                          extra={"request_id": request_id, "customer_id": customer_id, 
                                 "error": str(e), "processing_time": processing_time},
                          exc_info=True)  # Include stack trace
              return {"error": "Internal server error"}
      ```

examples:
  - input: |
      # Bad: Minimal logging without context
      def process_order(order_id):
          logger.info("Processing order")
          try:
              order = get_order(order_id)
              process_payment(order)
              update_inventory(order)
              send_confirmation(order)
              logger.info("Order processed")
              return True
          except Exception as e:
              logger.error("Error processing order")
              return False
    output: |
      # Good: Comprehensive logging with context
      def process_order(order_id: str) -> bool:
          """Process a customer order.
          
          Args:
              order_id: The ID of the order to process
              
          Returns:
              True if the order was processed successfully, False otherwise
          """
          logger.info(f"Processing order {order_id}", extra={"order_id": order_id})
          start_time = time.time()
          
          try:
              # Get order details
              order = get_order(order_id)
              customer_id = order.get("customer_id")
              order_total = order.get("total")
              logger.info(f"Retrieved order {order_id} for customer {customer_id}, total: ${order_total:.2f}",
                         extra={"order_id": order_id, "customer_id": customer_id, "total": order_total})
              
              # Process payment
              payment_start = time.time()
              payment_result = process_payment(order)
              payment_time = time.time() - payment_start
              logger.info(f"Payment processed for order {order_id} in {payment_time:.2f} seconds",
                         extra={"order_id": order_id, "payment_time": payment_time})
              
              # Update inventory
              inventory_start = time.time()
              inventory_result = update_inventory(order)
              inventory_time = time.time() - inventory_start
              logger.info(f"Inventory updated for order {order_id} in {inventory_time:.2f} seconds",
                         extra={"order_id": order_id, "inventory_time": inventory_time})
              
              # Send confirmation
              send_confirmation(order)
              
              total_time = time.time() - start_time
              logger.info(f"Order {order_id} processed successfully in {total_time:.2f} seconds",
                         extra={"order_id": order_id, "processing_time": total_time})
              return True
          except PaymentError as e:
              logger.error(f"Payment error for order {order_id}: {str(e)}",
                          extra={"order_id": order_id, "error": str(e)})
              return False
          except InventoryError as e:
              logger.error(f"Inventory error for order {order_id}: {str(e)}",
                          extra={"order_id": order_id, "error": str(e)})
              return False
          except Exception as e:
              logger.error(f"Unexpected error processing order {order_id}: {str(e)}",
                          extra={"order_id": order_id, "error": str(e)},
                          exc_info=True)  # Include stack trace
              return False
  
  - input: |
      # Bad: No performance metrics or structured logging
      def sync_data():
          logger.info("Starting data sync")
          data = fetch_remote_data()
          for item in data:
              process_item(item)
          logger.info("Data sync complete")
    output: |
      # Good: With performance metrics and structured logging
      def sync_data() -> Dict[str, Any]:
          """Synchronize data from remote source.
          
          Returns:
              Statistics about the sync operation
          """
          sync_id = str(uuid.uuid4())
          logger.info("Starting data sync", 
                     extra={"sync_id": sync_id, "operation": "data_sync", "status": "started"})
          
          start_time = time.time()
          stats = {
              "sync_id": sync_id,
              "items_processed": 0,
              "items_failed": 0,
              "items_skipped": 0
          }
          
          try:
              # Fetch data with timing
              fetch_start = time.time()
              data = fetch_remote_data()
              fetch_time = time.time() - fetch_start
              
              data_count = len(data)
              logger.info(f"Retrieved {data_count} items in {fetch_time:.2f} seconds", 
                         extra={"sync_id": sync_id, "item_count": data_count, "fetch_time": fetch_time})
              
              # Process items with progress logging
              for i, item in enumerate(data):
                  try:
                      # Log progress every 100 items
                      if i > 0 and i % 100 == 0:
                          elapsed = time.time() - start_time
                          items_per_second = i / elapsed
                          logger.info(f"Processed {i}/{data_count} items ({items_per_second:.1f} items/sec)",
                                     extra={"sync_id": sync_id, "progress": i, "total": data_count, 
                                            "items_per_second": items_per_second})
                      
                      process_item(item)
                      stats["items_processed"] += 1
                  except ItemSkippedError:
                      stats["items_skipped"] += 1
                  except Exception as e:
                      logger.warning(f"Failed to process item {item.get('id')}: {str(e)}",
                                   extra={"sync_id": sync_id, "item_id": item.get('id'), "error": str(e)})
                      stats["items_failed"] += 1
              
              # Log completion with statistics
              total_time = time.time() - start_time
              stats["total_time"] = total_time
              stats["items_per_second"] = stats["items_processed"] / total_time if total_time > 0 else 0
              
              logger.info(f"Data sync complete: {stats['items_processed']} processed, "
                         f"{stats['items_failed']} failed, {stats['items_skipped']} skipped in {total_time:.2f} seconds",
                         extra={"sync_id": sync_id, "operation": "data_sync", "status": "completed", **stats})
              
              return stats
          except Exception as e:
              total_time = time.time() - start_time
              logger.error(f"Data sync failed after {total_time:.2f} seconds: {str(e)}",
                          extra={"sync_id": sync_id, "operation": "data_sync", "status": "failed", 
                                 "error": str(e), "total_time": total_time},
                          exc_info=True)
              stats["error"] = str(e)
              stats["total_time"] = total_time
              return stats

metadata:
  priority: high
  version: 1.0
</rule> 